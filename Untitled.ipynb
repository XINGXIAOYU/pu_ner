{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1973\n",
      "745\n",
      "74\n",
      "38\n",
      "{'FLORIDA', 'Glamorgan', 'PUK', 'SAN FRANCISCO', 'Manchester United', 'CLEVELAND', 'PHILADELPHIA', 'Senate', 'USDA', 'Ajax', 'COLORADO', 'Norilsk', 'RUGBY UNION', 'CHICAGO', 'Essex', 'Santa Fe', 'Ajax Amsterdam', 'Nantes', 'MILWAUKEE', 'Kent', 'SEATTLE', 'Warwickshire', 'Honda', 'Cofinec', 'OSCE', 'PITTSBURGH', 'Newcastle', 'LOS ANGELES', 'PSV', 'SAN DIEGO', 'EU', 'Worcestershire', 'BOSTON', 'ST LOUIS', 'U.N.', 'Benetton', 'TORONTO', 'CALIFORNIA', 'HOUSTON', 'Barrick', 'London Newsroom', 'Interfax', 'Leeds', 'BALTIMORE', 'Liverpool', 'WTO', 'PKK', 'European Union', 'DETROIT', 'Karlsruhe', 'KDP', 'NATO', 'Leicestershire', 'ATLANTA', 'Nice', 'OAKLAND', 'United Nations', 'KANSAS CITY', 'BOJ', 'Samsung', 'Yorkshire', 'PSV Eindhoven', 'MINNESOTA', 'CINCINNATI', 'TEXAS', 'Reuters', 'OB', 'Lens', 'Bradford', 'MONTREAL', 'NEW YORK', 'Hampshire', 'Real Madrid', 'Surrey'}\n"
     ]
    }
   ],
   "source": [
    "d1 = set()\n",
    "d2 = set()\n",
    "d3 = set()\n",
    "d4 = set()\n",
    "with open(\"dictionary/person.txt\",\"r\") as fw:\n",
    "    for line in fw:\n",
    "        line = line.strip()\n",
    "        d1.add(line)\n",
    "with open(\"dictionary/location.txt\",\"r\") as fw:\n",
    "    for line in fw:\n",
    "        line = line.strip()\n",
    "        d2.add(line)\n",
    "with open(\"dictionary/organization.txt\",\"r\") as fw:\n",
    "    for line in fw:\n",
    "        line = line.strip()\n",
    "        d3.add(line)\n",
    "with open(\"dictionary/misc.txt\",\"r\") as fw:\n",
    "    for line in fw:\n",
    "        line = line.strip()\n",
    "        d4.add(line)\n",
    "        \n",
    "print(len(person_dict))\n",
    "print(len(location_dict))\n",
    "print(len(org_dict))\n",
    "print(len(misc_dict))\n",
    "        \n",
    "person_dict = d1-d2-d3-d4\n",
    "location_dict = d2-d1-d3-d4\n",
    "org_dict = d3-d1-d2-d4\n",
    "misc_dict = d4-d1-d2-d3\n",
    "print(org_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1973\n",
      "745\n",
      "74\n",
      "38\n",
      "{'FLORIDA', 'Glamorgan', 'PUK', 'SAN FRANCISCO', 'Manchester United', 'CLEVELAND', 'PHILADELPHIA', 'Senate', 'USDA', 'Ajax', 'COLORADO', 'Norilsk', 'RUGBY UNION', 'CHICAGO', 'Essex', 'Santa Fe', 'Ajax Amsterdam', 'Nantes', 'MILWAUKEE', 'Kent', 'SEATTLE', 'Warwickshire', 'Honda', 'Cofinec', 'OSCE', 'PITTSBURGH', 'Newcastle', 'LOS ANGELES', 'PSV', 'SAN DIEGO', 'EU', 'Worcestershire', 'BOSTON', 'ST LOUIS', 'U.N.', 'Benetton', 'TORONTO', 'CALIFORNIA', 'HOUSTON', 'Barrick', 'London Newsroom', 'Interfax', 'Leeds', 'BALTIMORE', 'Liverpool', 'WTO', 'PKK', 'European Union', 'DETROIT', 'Karlsruhe', 'KDP', 'NATO', 'Leicestershire', 'ATLANTA', 'Nice', 'OAKLAND', 'United Nations', 'KANSAS CITY', 'BOJ', 'Samsung', 'Yorkshire', 'PSV Eindhoven', 'MINNESOTA', 'CINCINNATI', 'TEXAS', 'Reuters', 'OB', 'Lens', 'Bradford', 'MONTREAL', 'NEW YORK', 'Hampshire', 'Real Madrid', 'Surrey'}\n"
     ]
    }
   ],
   "source": [
    "print(len(person_dict))\n",
    "print(len(location_dict))\n",
    "print(len(org_dict))\n",
    "print(len(misc_dict))\n",
    "print(org_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"dictionary/person.txt\",\"w\") as fw:\n",
    "    for name in person_dict:\n",
    "        fw.write(name+\"\\n\")\n",
    "with open(\"dictionary/location.txt\",\"w\") as fw:\n",
    "    for name in location_dict:\n",
    "        fw.write(name+\"\\n\")\n",
    "with open(\"dictionary/organization.txt\",\"w\") as fw:\n",
    "    for name in org_dict:\n",
    "        fw.write(name+\"\\n\")\n",
    "with open(\"dictionary/misc.txt\",\"w\") as fw:\n",
    "    for name in misc_dict:\n",
    "        fw.write(name+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1696\n",
      "39007\n",
      "1136\n",
      "560\n"
     ]
    }
   ],
   "source": [
    "# split wikigold\n",
    "sentences = []\n",
    "with codecs.open(\"wikigold/wikigold.conll.txt\", \"r\",encoding='utf-8', errors='ignore') as fw: \n",
    "    sentence = []\n",
    "    for line in fw:\n",
    "        if len(line) == 0 or '-DOCSTART' in line or line[0] == '\\n':\n",
    "            if len(sentence) > 0:\n",
    "                sentences.append(sentence)\n",
    "                sentence = []\n",
    "            continue\n",
    "        else:\n",
    "            splits = line.split(' ')\n",
    "            sentence.append([splits[0].strip(), splits[-1].strip(), np.zeros(4)])\n",
    "\n",
    "    if len(sentence) > 0:\n",
    "        sentences.append(sentence)\n",
    "        sentence = []\n",
    "        \n",
    "print(len(sentences))\n",
    "\n",
    "a = 0\n",
    "for i in sentences:\n",
    "    a+=len(i)\n",
    "print(a)\n",
    "\n",
    "with open(\"data/wikigold.txt\",\"w\") as fw:\n",
    "    for sentence in sentences:\n",
    "        for word, label, tagIdxList in sentence:\n",
    "            labeled = 0\n",
    "            fw.write(word + \" \" + label + \" \" + str(labeled) + \"\\n\")\n",
    "        fw.write(\"\\n\")\n",
    "        \n",
    "        \n",
    "from sklearn.model_selection import train_test_split\n",
    "train,test = train_test_split(sentences, test_size=0.33, random_state=42)\n",
    "print(len(train))\n",
    "print(len(test))\n",
    "\n",
    "with open(\"data/wikigold.train.txt\",\"w\") as fw:\n",
    "    for sentence in train:\n",
    "        for word, label, tagIdxList in sentence:\n",
    "            labeled = 0\n",
    "            fw.write(word + \" \" + label + \" \" + str(labeled) + \"\\n\")\n",
    "        fw.write(\"\\n\")\n",
    "        \n",
    "with open(\"data/wikigold.test.txt\",\"w\") as fw:\n",
    "    for sentence in test:\n",
    "        for word, label, tagIdxList in sentence:\n",
    "            labeled = 0\n",
    "            fw.write(word + \" \" + label + \" \" + str(labeled) + \"\\n\")\n",
    "        fw.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "619\n",
      "7545\n"
     ]
    }
   ],
   "source": [
    "# split web dataset\n",
    "import os\n",
    "import numpy as np\n",
    "import codecs\n",
    "files = os.listdir(\"WebpagesColumns\")\n",
    "sentences = []\n",
    "for file in files:\n",
    "    path = os.path.join(\"WebpagesColumns\",file)\n",
    " \n",
    "    with codecs.open(path, \"r\",encoding='utf-8', errors='ignore') as fw:\n",
    "            sentence = []\n",
    "            for line in fw:\n",
    "                if len(line) == 0 or '-DOCSTART' in line or line[0] == '\\n':\n",
    "                    if len(sentence) > 0:\n",
    "                        sentences.append(sentence)\n",
    "                        sentence = []\n",
    "                    continue\n",
    "                else:\n",
    "                    splits = line.split('\\t')\n",
    "                    sentence.append([splits[0].strip(), splits[5].strip(), np.zeros(4)])\n",
    "\n",
    "            if len(sentence) > 0:\n",
    "                sentences.append(sentence)\n",
    "                sentence = []\n",
    "\n",
    "print(len(sentences))\n",
    "a = 0\n",
    "for i in sentences:\n",
    "    a+=len(i)\n",
    "print(a)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "414\n",
      "205\n"
     ]
    }
   ],
   "source": [
    "with open(\"data/webpages.txt\",\"w\") as fw:\n",
    "    for sentence in sentences:\n",
    "        for label, word, tagIdxList in sentence:\n",
    "            labeled = 0\n",
    "            fw.write(word + \" \" + label + \" \" + str(labeled) + \"\\n\")\n",
    "        fw.write(\"\\n\")\n",
    "        \n",
    "from sklearn.model_selection import train_test_split\n",
    "train,test = train_test_split(sentences, test_size=0.33, random_state=42)\n",
    "print(len(train))\n",
    "print(len(test))\n",
    "\n",
    "with open(\"data/webpages.train.txt\",\"w\") as fw:\n",
    "    for sentence in train:\n",
    "        for label, word, tagIdxList in sentence:\n",
    "            labeled = 0\n",
    "            fw.write(word + \" \" + label + \" \" + str(labeled) + \"\\n\")\n",
    "        fw.write(\"\\n\")\n",
    "        \n",
    "with open(\"data/webpages.test.txt\",\"w\") as fw:\n",
    "    for sentence in test:\n",
    "        for label, word, tagIdxList in sentence:\n",
    "            labeled = 0\n",
    "            fw.write(word + \" \" + label + \" \" + str(labeled) + \"\\n\")\n",
    "        fw.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
