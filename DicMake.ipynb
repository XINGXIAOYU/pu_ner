{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# @Time    : 2018/7/25 09:28\n",
    "# @Author  : Xiaoyu Xing\n",
    "# @File    : make_dic.py\n",
    "\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from pandas import DataFrame\n",
    "\n",
    "\n",
    "def readfile(filename):\n",
    "    with open(filename, \"r\") as fw:\n",
    "        sentences = []\n",
    "        sentence = []\n",
    "        for line in fw:\n",
    "            if len(line) == 0 or line.startswith('-DOCSTART') or line[0] == '\\n':\n",
    "                if len(sentence) > 0:\n",
    "                    sentences.append(sentence)\n",
    "                    sentence = []\n",
    "                continue\n",
    "            else:\n",
    "                splits = line.split(' ')\n",
    "                sentence.append([splits[0].strip(), splits[1].strip()])\n",
    "\n",
    "        if len(sentence) > 0:\n",
    "            sentences.append(sentence)\n",
    "            sentence = []\n",
    "\n",
    "        return sentences\n",
    "\n",
    "\n",
    "personDic = defaultdict(int)\n",
    "locDic = defaultdict(int)\n",
    "orgDic = defaultdict(int)\n",
    "miscDic = defaultdict(int)\n",
    "\n",
    "sentences1 = readfile(\"data/twitter/train.txt\")\n",
    "sentences2 = readfile(\"data/twitter/valid.txt\")\n",
    "sentences3 = readfile(\"data/twitter/test.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addDic(sentences,personDic,locDic,orgDic,miscDic):\n",
    "    for sentence in sentences:\n",
    "        for i, (word, label) in enumerate(sentence):\n",
    "            phase = word\n",
    "            if label!='O':\n",
    "                splits = label.split(\"-\")\n",
    "                tag = splits[0]\n",
    "                entityLabel = splits[1]\n",
    "                if tag == 'B':\n",
    "                    j = i + 1\n",
    "                    while (j < len(sentence)):\n",
    "                        if sentence[j][1] != 'O':\n",
    "                            tag2 = sentence[j][1].split('-')[0]\n",
    "                            entityLabel2 = sentence[j][1].split('-')[1]\n",
    "                            if (tag2 == 'I' and entityLabel2 == entityLabel):\n",
    "                                phase = phase + \" \" + sentence[j][0]\n",
    "                                j += 1\n",
    "                                if j==len(sentence):\n",
    "                                    if entityLabel == \"PER\":\n",
    "                                        personDic[phase] += 1\n",
    "                                    elif entityLabel == \"LOC\":\n",
    "                                        locDic[phase] += 1\n",
    "                                    elif entityLabel == \"ORG\":\n",
    "                                        orgDic[phase] += 1\n",
    "                                    elif entityLabel ==\"MISC\":\n",
    "                                        miscDic[phase]+=1\n",
    "                                    break\n",
    "                            else:\n",
    "                                if entityLabel == \"PER\":\n",
    "                                    personDic[phase] += 1\n",
    "                                elif entityLabel == \"LOC\":\n",
    "                                    locDic[phase] += 1\n",
    "                                elif entityLabel == \"ORG\":\n",
    "                                    orgDic[phase] += 1\n",
    "                                elif entityLabel ==\"MISC\":\n",
    "                                    miscDic[phase]+=1\n",
    "                                break\n",
    "                        else:\n",
    "                            if entityLabel == \"PER\":\n",
    "                                personDic[phase] += 1\n",
    "                            elif entityLabel == \"LOC\":\n",
    "                                locDic[phase] += 1\n",
    "                            elif entityLabel == \"ORG\":\n",
    "                                orgDic[phase] += 1\n",
    "                            elif entityLabel ==\"MISC\":\n",
    "                                miscDic[phase]+=1\n",
    "                            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "addDic(sentences1,personDic,locDic,orgDic,miscDic)\n",
    "addDic(sentences2,personDic,locDic,orgDic,miscDic)\n",
    "addDic(sentences3,personDic,locDic,orgDic,miscDic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"dictionary/twitter/person.txt\",\"r\") as fw:\n",
    "    for line in fw:\n",
    "        line = line.strip()\n",
    "        personDic[line]+=1\n",
    "with open(\"dictionary/twitter/location.txt\",\"r\") as fw:\n",
    "    for line in fw:\n",
    "        line = line.strip()\n",
    "        locDic[line]+=1\n",
    "with open(\"dictionary/twitter/organization.txt\",\"r\") as fw:\n",
    "    for line in fw:\n",
    "        line = line.strip()\n",
    "        orgDic[line]+=1\n",
    "with open(\"dictionary/twitter/misc.txt\",\"r\") as fw:\n",
    "    for line in fw:\n",
    "        line = line.strip()\n",
    "        miscDic[line]+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_big_dic(dic,fileName):\n",
    "    with open(fileName,\"w\") as fw:\n",
    "        for key,value in dic.items():\n",
    "            fw.write(key+\"\\n\")\n",
    "make_big_dic(personDic,\"personBigDic.txt\")\n",
    "make_big_dic(locDic,\"locationBigDic.txt\")\n",
    "make_big_dic(orgDic,\"organizationBigDic.txt\")\n",
    "make_big_dic(orgDic,\"miscBigDic.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3044963005122368\n",
      "216\n",
      "3514\n",
      "['Kanye', 'Obama', 'Trump', 'Rihanna', 'John', 'Donald Trump', 'LeBron James', 'Robin Williams', 'Harry', 'Hillary Clinton', 'President Obama', 'Putin', 'Bruce Rauner', 'Taylor Swift', 'Jesus', 'Drake', 'Justin Bieber', 'Liam', 'Niall', 'Lloyd', 'Bob Marley', 'Justin', 'Kobe', 'Adam', 'Brandon Saad', 'Kanye West', 'Ted Cruz', 'Louis', 'Kendall Jenner', 'John Lennon', 'Ariana Grande', 'Kurt Cobain', 'Clinton', 'Joe', 'Messi', 'Chris Sale', 'Johnny Depp', 'Stuart Scott', 'Jimmy', 'Will Smith', 'Bruce', 'Pope Francis', 'Steve Jobs', 'Michelle', 'Kylie Jenner', 'Nelson Mandela', 'LeBron', 'Biden', 'Jon Stewart', 'Jack', 'David', 'Derrick Rose', 'Santa', 'Bill Murray', 'Steph Curry', 'James', 'Hillary', 'Nick', 'Jay Z', 'Kevin Durant', 'Angelina Jolie', 'Harry Potter', 'Picasso', 'Kobe Bryant', 'Bill Gates', 'Cardinal Francis George', 'Quinn', 'Blatter', 'Matthew', 'Bill', 'President Rajapaksa', 'Antoine Vermette', 'Ben Bishop', 'OBAMA', 'Roger', 'Bush', 'Andrew Luck', 'Martin Luther King', 'Archbishop Blase Cupich', 'Elizabeth Warren', 'Elvis', 'Chuck Bass', 'Michael Jackson', 'Jared Leto', 'Dean Smith', 'Maya Angelou', 'Martin Garrix', 'Robin', 'Kim Davis', 'Taylor', 'Banksy', 'Emma', 'James Harden', 'Paul', 'Jared', 'Jonathan Toews', 'Michael Jordan', 'Jeb Bush', 'Debbie', 'Nicki', 'Bey', 'Marilyn Monroe', 'Corey Crawford', 'Toews', 'Trayvon Martin', 'Alex', 'Anthony', 'Tim Duncan', 'Zayn', 'Joe Biden', 'Miley Cyrus', 'Lou Gehrig', 'Matt', 'Oscar Grant', 'Grace', 'Jay Cutler', 'Amanda', 'Chris Christie', 'Nigel Farage', 'Bob', 'Mother Teresa', 'Selena Gomez', 'Lil Kim', 'Jimmy Butler', 'Johnny', 'Richard Sherman', 'Jerry Collins', 'Brian', 'McConnell', 'Rick Perry', 'David Carr', 'Toni Morrison', 'Niall Horan', 'Sandy', 'Leonardo DiCaprio', 'Jameis Winston', 'Mayweather', 'Tim', 'Marianne Hale', 'Muhammad', 'Mitt Romney', 'Josh', 'Demi Lovato', 'Nate Archibald', 'Odin', 'Sarah Palin', 'Byron Maxwell', 'Steve Kerr', 'Demi', 'Emily', 'Rahm Emanuel', 'Timmy', 'Patrick Kane', 'Sandra Bullock', 'George Lucas', 'Evan Peters', 'Xavi', 'Vanessa Hudgens', 'Bradley', 'HARRY', 'Barack Obama', 'Brad', 'Kris', 'Ed Sheeran', 'Todd', 'Jim', 'Mark Buehrle', 'Anthony Davis', 'Caitlyn Jenner', 'Viola Davis', 'Martin', 'Sebastian Vettel', 'Peter', 'Roger Federer', 'Pekka Rinne', 'Nicki Minaj', 'Mario', 'Sen', 'Sophie', 'Stephen', 'Jordan Spieth', 'Jennifer Lawrence', 'Robert', 'Diana', 'Michael', 'Ernie Banks', 'Bryan Bickell', 'Marco Rubio', 'Andrew Shaw', 'Lauren Hill', 'Daniel', 'Jensen', 'Eminem', 'Loretta Lynch', 'Dez Bryant', 'Sarah', 'Kyrie Irving', 'Albert Einstein', 'Lena Dunham', 'Kevin', 'Henry', 'Ronaldo', 'Erin', 'Kevin White', 'Ross', 'James Franco', 'Ryan Giggs', 'Nash', 'Allen Iverson', 'Harper', 'Lionel Messi', 'Kane', 'Tom Brady', 'Kim', 'Alshon Jeffrey', 'Adam LaRoche']\n"
     ]
    }
   ],
   "source": [
    "pdf = DataFrame()\n",
    "pdf[\"name\"] = list(personDic.keys())\n",
    "pdf[\"value\"] = list(personDic.values())\n",
    "pdf = pdf.sort_values(by=['value'],ascending=False)\n",
    "print(pdf[\"value\"].mean())\n",
    "highPName = pdf[pdf.value>=3][\"name\"].tolist()\n",
    "print(len(highPName))\n",
    "print(len(pdf))\n",
    "pset = highPName\n",
    "print(highPName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.730228823765556\n",
      "140\n",
      "2491\n",
      "           name  value\n",
      "113     Chicago     50\n",
      "60       London     42\n",
      "132          US     33\n",
      "146     America     32\n",
      "68       Russia     30\n",
      "280       China     28\n",
      "87   California     22\n",
      "229         NYC     20\n",
      "692      France     19\n",
      "134      Mexico     19\n",
      "93        Paris     19\n",
      "78    Australia     18\n",
      "182       Japan     18\n",
      "458          UK     18\n",
      "332    New York     17\n",
      "578      Canada     17\n",
      "178         USA     16\n",
      "180      Boston     16\n",
      "17      Germany     16\n",
      "689     Florida     15\n",
      "851      Hawaii     15\n",
      "149       India     15\n",
      "165      Europe     15\n",
      "49        Italy     15\n",
      "772       Texas     14\n",
      "29           NY     14\n",
      "486      Africa     13\n",
      "65   Washington     13\n",
      "163      Brazil     13\n",
      "293      Greece     13\n",
      "275        Iran     12\n",
      "502     Arizona     12\n",
      "3     Baltimore     12\n",
      "4            LA     12\n",
      "137       Kenya     11\n",
      "233    Illinois     11\n",
      "817       Syria     11\n",
      "216       Maine     11\n",
      "429   Hong Kong     11\n",
      "516         the     10\n"
     ]
    }
   ],
   "source": [
    "ldf = DataFrame()\n",
    "ldf[\"name\"] = list(locDic.keys())\n",
    "ldf[\"value\"] = list(locDic.values())\n",
    "ldf = ldf.sort_values(by=['value'],ascending=False)\n",
    "print(ldf[\"value\"].mean())\n",
    "highLName = ldf[ldf.value>=5][\"name\"].tolist()\n",
    "print(len(highLName))\n",
    "print(len(ldf))\n",
    "lset = highLName\n",
    "print(ldf[:40])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4657933042212519\n",
      "104\n",
      "1374\n",
      "            name  value\n",
      "77   #Blackhawks     57\n",
      "64       Twitter     25\n",
      "84      Facebook     18\n",
      "41         Apple     18\n",
      "129       #Preds     17\n",
      "39        #Bears     16\n",
      "90        Google     16\n",
      "54           GOP     15\n",
      "240       Amazon     12\n",
      "217       Disney     11\n",
      "181          the     10\n",
      "87          NASA     10\n",
      "5         Senate     10\n",
      "79       Netflix      9\n",
      "15           BBC      9\n",
      "75          ISIS      9\n",
      "38         #MUFC      8\n",
      "197      YouTube      7\n",
      "280        Bulls      6\n",
      "112        #Cubs      6\n",
      "667          EXO      6\n",
      "257      Marines      6\n",
      "80           CNN      6\n",
      "23     White Sox      6\n",
      "230           UN      5\n",
      "52     Liverpool      5\n",
      "78    Blackhawks      5\n",
      "211     Patriots      5\n",
      "124       #Bulls      5\n",
      "312          #fb      5\n",
      "247    Microsoft      5\n",
      "374       #Ducks      5\n",
      "150          EMC      5\n",
      "192      Harvard      5\n",
      "648         Sony      5\n",
      "950        Spurs      5\n",
      "29           NHL      5\n",
      "272    Instagram      5\n",
      "276         ESPN      5\n",
      "106      Samsung      5\n"
     ]
    }
   ],
   "source": [
    "odf = DataFrame()\n",
    "odf[\"name\"] = list(orgDic.keys())\n",
    "odf[\"value\"] = list(orgDic.values())\n",
    "odf = odf.sort_values(by=['value'],ascending=False)\n",
    "print(odf[\"value\"].mean())\n",
    "highOName = odf[odf.value>=3][\"name\"].tolist()\n",
    "print(len(highOName))\n",
    "print(len(odf))\n",
    "oset = highOName\n",
    "print(odf[:40])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all freq 1695\n",
      "abbr freq 397\n",
      "unique name 465\n",
      "unique abbr 33\n"
     ]
    }
   ],
   "source": [
    "odf = DataFrame()\n",
    "odf[\"name\"] = list(orgDic.keys())\n",
    "odf[\"value\"] = list(orgDic.values())\n",
    "odf = odf.sort_values(by=['value'],ascending=False)\n",
    "print(odf[\"value\"].mean())\n",
    "highOName = odf[odf.value>=8][\"name\"].tolist()\n",
    "print(len(highOName))\n",
    "print(len(odf))\n",
    "oset = highOName[:200]\n",
    "allName = orgDic.keys()\n",
    "abbrCount = 0\n",
    "freqCount  =0\n",
    "allCount = 0\n",
    "for key,value in orgDic.items():\n",
    "    allCount+=value\n",
    "print(\"all freq {}\".format(allCount))\n",
    "abbrWords = []\n",
    "for i in allName:\n",
    "    word = i\n",
    "    if  word.isupper():\n",
    "        abbrCount+=1\n",
    "        abbrWords.append(word)\n",
    "        freqCount+=orgDic[word]\n",
    "print(\"abbr freq {}\".format(freqCount))\n",
    "print(\"unique name {}\".format(len(allName)))\n",
    "print(\"unique abbr {}\".format(abbrCount))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1811367895065583\n",
      "139\n",
      "1601\n",
      "                        name  value\n",
      "89                       the     16\n",
      "7                        NBA     15\n",
      "204                      NFL     11\n",
      "28                 Christmas     11\n",
      "46                 Instagram      8\n",
      "134                World Cup      8\n",
      "327              Stanley Cup      8\n",
      "0                    Olympic      6\n",
      "440                Star Wars      6\n",
      "41                    Batman      6\n",
      "493                     #NFL      6\n",
      "34                     Ebola      5\n",
      "681               Super Bowl      5\n",
      "663                 Olympics      5\n",
      "765         Independence Day      5\n",
      "736                      The      5\n",
      "414                      #F1      5\n",
      "225                   iPhone      5\n",
      "9                  Minecraft      5\n",
      "220             Thanksgiving      5\n",
      "213                 New Year      5\n",
      "115                    Oscar      5\n",
      "430   The Seth Leibsohn Show      4\n",
      "230          Boston Marathon      4\n",
      "50                      Siri      4\n",
      "205                     #CGE      4\n",
      "121                  Twitter      4\n",
      "81                 Halloween      4\n",
      "479                Pinterest      4\n",
      "875                  Android      4\n",
      "24                    Oscars      4\n",
      "763             Harry Potter      4\n",
      "576                   #Ebola      3\n",
      "655                    Orion      3\n",
      "483              Google Maps      3\n",
      "572                ObamaCare      3\n",
      "71                      #AFP      3\n",
      "1101               Obamacare      3\n",
      "593                       IT      3\n",
      "619                     iPad      3\n"
     ]
    }
   ],
   "source": [
    "mdf = DataFrame()\n",
    "mdf[\"name\"] = list(miscDic.keys())\n",
    "mdf[\"value\"] = list(miscDic.values())\n",
    "mdf = mdf.sort_values(by=['value'],ascending=False)\n",
    "print(mdf[\"value\"].mean())\n",
    "highMName = mdf[mdf.value>=2][\"name\"].tolist()\n",
    "print(len(highMName))\n",
    "print(len(mdf))\n",
    "mset = highMName\n",
    "print(mdf[:40])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "with open(\"dictionary/twitter/person2.txt\",\"w\") as fw:\n",
    "    for name in pset:\n",
    "        fw.write(name+\"\\n\")\n",
    "with open(\"dictionary/twitter/location2.txt\",\"w\") as fw:\n",
    "    for name in lset:\n",
    "        fw.write(name+\"\\n\")\n",
    "with open(\"dictionary/twitter/organization2.txt\",\"w\") as fw:\n",
    "    for name in oset:\n",
    "        fw.write(name+\"\\n\")\n",
    "with open(\"dictionary/twitter/misc2.txt\",\"w\") as fw:\n",
    "    for name in mset:\n",
    "        fw.write(name+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "orgDic = defaultdict(int)\n",
    "miscDic = defaultdict(int)\n",
    "personDic = defaultdict(int)\n",
    "locDic = defaultdict(int)\n",
    "\n",
    "with open(\"dictionary/twitter/organization.txt\",\"r\") as fw:\n",
    "    for line in fw:\n",
    "        line = line.strip()\n",
    "        orgDic[line]+=1\n",
    "with open(\"dictionary/twitter/misc.txt\",\"r\") as fw:\n",
    "    for line in fw:\n",
    "        line = line.strip()\n",
    "        miscDic[line]+=1\n",
    "        \n",
    "with open(\"dictionary/twitter/person.txt\",\"r\") as fw:\n",
    "    for line in fw:\n",
    "        line = line.strip()\n",
    "        personDic[line]+=1\n",
    "        \n",
    "with open(\"dictionary/twitter/location.txt\",\"r\") as fw:\n",
    "    for line in fw:\n",
    "        line = line.strip()\n",
    "        locDic[line]+=1\n",
    "        \n",
    "with open(\"dictionary/twitter/organization2.txt\",\"r\") as fw:\n",
    "    for line in fw:\n",
    "        line = line.strip()\n",
    "        orgDic[line]+=1\n",
    "with open(\"dictionary/twitter/misc2.txt\",\"r\") as fw:\n",
    "    for line in fw:\n",
    "        line = line.strip()\n",
    "        miscDic[line]+=1\n",
    "        \n",
    "with open(\"dictionary/twitter/person2.txt\",\"r\") as fw:\n",
    "    for line in fw:\n",
    "        line = line.strip()\n",
    "        personDic[line]+=1\n",
    "        \n",
    "with open(\"dictionary/twitter/location2.txt\",\"r\") as fw:\n",
    "    for line in fw:\n",
    "        line = line.strip()\n",
    "        locDic[line]+=1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "357\n",
      "342\n",
      "{'Serbian', 'Selma', 'Antarctic', 'Mahoran', 'Mongolian', 'Austrian', 'Islamist', 'Czech', 'Swazi', 'Algerian', 'iPhone', 'Olympics', 'Grammys', 'Tunisian', 'Palauan', 'Thai', 'Bonaire', 'Papua New Guinean', '#tahoe', 'Herzegovinian', 'Ebola', 'Vikings', 'The', 'Gabonese', 'Caymanian', 'Kittitian', '#LLWS', 'Maggie Simpson', 'Jordanian', 'PS4', 'Macanese', 'Finnish', 'Sahraouian', 'Iranian', 'iPod', 'Slovenian', 'Egyptian', 'Motswana', 'Abkhaz', 'Laotian', 'Oscar', 'Android', 'Sri Lankan', 'Singapore', 'Comorian', 'Nepalese', 'The Breakfast Club', 'Saudi', 'Tokelauan', 'Ferrari', '#Ebola', 'Kirghiz', 'Guinean', 'Zambian', 'The Seth Leibsohn Show', 'Jan Mayen', 'Belizean', 'Waze', 'Niuean', 'Monégasque', 'World War I', '#Twitter', 'Comoran', 'Dominican', 'Congolese', 'Kazakhstani', 'Harry Potter', 'Qatari', 'Memorial Day', 'Saint Lucian', 'Kardashians', 'Djiboutian', 'Malinese', 'Daryl', '#Lolla', 'Emiri', 'Belarusian', 'Manx', 'Maltese', 'Swedish', '#Christmas', 'Greek', 'Heard Island', 'Sammarinese', 'Palestinian', 'Luxembourgish', 'Faroese', 'Adjectivals', 'Mozambican', '#VMAs', 'Beninese', 'Rwandan', 'Cypriot', 'Star Wars', 'Slovene', 'Omani', 'Instagram', 'Tajikistani', 'Lombardi Trophy', 'Tanzanian', 'Emirati', '5lb Hershey bar', 'Burmese', 'Bosnian', '#LDS', 'Barbudan', 'Pakistani', 'Surinamese', 'Namibian', 'Basotho', 'Vatican', 'Game of Thrones', 'Pinterest', 'Eritrean', 'Georgian', 'Grenadian', 'Vincentian', 'Flash Flood Warning', 'European', 'AIDS', 'MERS', 'Tuvaluan', 'Yemeni', 'NFL', 'BIOT', 'Equatoguinean', 'Luxembourg', 'MBA', 'Sahrawian', 'Spike', 'Bahamian', 'Swiss', 'Olympic', '#NYE', 'Miquelonnais', 'Papuan', 'Anguillan', 'iCloud', 'South African', 'Microsoft Exchange', 'Kyrgyzstani', 'American Samoan', 'Iraqi', 'PSPA competition', 'Monacan', 'Martinican', 'Mickey Mouse', 'FT', 'Sint Maarten', 'Hellenic', 'Bhutanese', '#Hipstamatic', 'New Years', 'Burundian', 'Antiguan', 'Persian', 'Magyar', 'Frozen', 'Lebanese', 'Azeri', 'Angolan', 'Impala', 'Turkmen', 'Data Lake', 'Macedonian', 'Fifty Shades of Grey', 'Micronesian', 'Mary Poppins', 'Afghan', 'Mauritanian', 'Saba', 'Israeli', 'Martiniquais', 'Togolese', 'Bahraini', 'Eurovision', 'Fox News', 'Bouvet Island', 'FaceTime', 'Halloween', 'Futunan', 'Liberian', 'Vanuatuan', 'Jamaican', 'Orion', 'Chadian', 'Starbucks Card', 'WhatsApp', 'Saint-Martinoise', 'Azerbaijani', 'Guyanese', 'Ecuadorian', 'Ugandan', 'Armenian', 'EST', 'Venezuelan', 'Nepali', 'Kosovar', 'Vietnamese', 'Disney', 'THE BEST YEARS OF OUR LIVES', 'iTunes', '#CGE', 'Grand Prix', 'Labor Day', 'SAP Web IDE', 'Malaysian', 'Moroccan', 'Moldovan', 'Guadeloupe', 'CO2', '#Pashto', 'Malawian', 'New Year', 'Andorran', 'North Korean', 'Siri', 'Danish', 'Obamacare', 'Cook Island', 'Bolivian', 'Réunionnais', 'Svalbard', '#hpmkt', 'Emirian', 'Greenlandic', 'Kuwaiti', 'Maldivian', '#F1', 'Nigerien', 'Samoan', 'Sahrawi', 'Netherlandic', 'Cambodian', 'Polish', 'HIV', 'Stanley Cup', 'Ivorian', 'Texworld Paris', 'grammy', 'Gambian', 'Cameroonian', 'Chilean', 'Estonian', 'TVD', 'Fijian', 'Burkinabé', 'Thanksgiving', 'Kosovan', 'Air Jordan', 'Kazakh', 'Jurassic World', 'Libyan', 'Twitter', 'South Sudanese', 'Albanian', 'Singaporean', 'Seychellois', 'Uzbekistani', 'Taiwanese', 'Windows 10', 'Montserratian', 'FIELD TRIP FRIDAY', 'Pi Day', 'American Sniper', 'Chinese', 'FBS', '#Poldark', 'Curaçaoan', 'Somali', 'Sudanese', 'South Georgia', 'Abkhazian', 'Sugar Bowl', 'Lithuanian', '#Mormon', 'Statian', 'Botswanan', 'Tongan', 'Japanese', 'Indonesian', '#Howard', 'Guamanian', 'Bruneian', 'World Cup', 'Kyrgyz', 'Teen Wolf', 'Final Dress Rehearsal of MACBETH', 'Hungarian', 'Coco', 'Dutch', '#Catholic', 'MTV VMAs', 'Somalilander', 'NBA', 'Kirgiz', 'Malian', 'Independence Day', 'Italian', 'Boston Marathon', 'Cabo Verdean', 'soap opera', 'Marshallese', 'the', 'Pluto', 'Wallisian', 'Uzbek', 'Batman', '#Facebook', 'Nauruan', '#Easter', '#Arquitectura', 'Earth Day', 'Bulgarian', 'weibo', 'British', 'Ghostbusters', 'Malagasy', 'Aruban', 'Minecraft', 'Ghanaian', '#Vikings', 'iPad', 'WWII', 'Scottish', 'Lion King', 'Latvian', 'Central African', 'Christmas', 'Croatian', 'Ethiopian', 'Mauritian', 'Oscars', 'Icelandic', 'Google Maps', 'Kenyan', '#NFL', 'Turkish', 'Slovak', 'Xmas', 'Romanian', 'Montenegrin', '#LGBT', '#SDCC', 'Kursk', 'Fruitvale Station', 'Beninois', 'Senegalese', 'Belgian', 'Disney Channel', 'Lettish', 'Nevisian', 'Cuban', 'Norwegian', 'Syrian', 'ObamaCare', 'Confederate flag'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "set1 = set(personDic.keys())\n",
    "set2 = set(locDic.keys())\n",
    "set3 = set(orgDic.keys())\n",
    "set4 = set(miscDic.keys())\n",
    "\n",
    "\n",
    "    \n",
    "print(len(set4))\n",
    "    \n",
    "a = set1-set2-set3-set4\n",
    "b = set2-set1-set3-set4\n",
    "c = set3-set1-set2-set4\n",
    "d = set4-set1-set2-set3\n",
    "\n",
    "print(len(d))\n",
    "\n",
    "print(set4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"dictionary/twitter/person3.txt\",\"w\") as fw:\n",
    "    for i in a:\n",
    "        fw.write(i+\"\\n\")\n",
    "with open(\"dictionary/twitter/location3.txt\",\"w\") as fw:\n",
    "    for i in b:\n",
    "        fw.write(i+\"\\n\")\n",
    "with open(\"dictionary/twitter/organization3.txt\",\"w\") as fw:\n",
    "    for i in c:\n",
    "        fw.write(i+\"\\n\")\n",
    "with open(\"dictionary/twitter/misc3.txt\",\"w\") as fw:\n",
    "    for i in d:\n",
    "        fw.write(i+\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
